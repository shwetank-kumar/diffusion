{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('high')\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import autoaugment\n",
    "from einops import rearrange, repeat\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomResizedCrop((32, 32), scale=(0.8, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.5071, 0.4867, 0.4408],\n",
    "        std=[0.2675, 0.2565, 0.2761]\n",
    "    )\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.5071, 0.4867, 0.4408],\n",
    "        std=[0.2675, 0.2565, 0.2761]\n",
    "    )\n",
    "])\n",
    "\n",
    "# cifar100 = torchvision.datasets.CIFAR100(root='../datasets', download=True)\n",
    "cifar100_train = torchvision.datasets.CIFAR100(\n",
    "    root='../datasets', \n",
    "    train=True,  # Important: Specify train=True\n",
    "    transform=train_transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "cifar100_val = torchvision.datasets.CIFAR100(\n",
    "    root='../datasets', \n",
    "    train=False,  # Using training set for validation\n",
    "    transform=val_transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    cifar100_train, batch_size=batch_size, shuffle=True, num_workers=12, pin_memory=True \n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    cifar100_val, batch_size=batch_size, shuffle=False, num_workers=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of images and their corresponding names\n",
    "dataiter = iter(train_loader)\n",
    "images, targets = next(dataiter)\n",
    "images = images.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(images, is_patches=False, patch_grid=None, line_width=1, line_color=(0, 0, 0)):\n",
    "    \"\"\"\n",
    "    Display a batch of images or image patches with grid lines separating patches.\n",
    "    \"\"\"\n",
    "    # Convert to CPU and make a copy\n",
    "    images = images.cpu().clone().detach()\n",
    "\n",
    "    # Denormalize\n",
    "    mean = torch.tensor([0.5071, 0.4867, 0.4408]).view(1, 3, 1, 1).to(images.device)\n",
    "    std = torch.tensor([0.2675, 0.2565, 0.2761]).view(1, 3, 1, 1).to(images.device)\n",
    "    \n",
    "    # Denormalize: undo the normalization\n",
    "    images = images * std + mean\n",
    "\n",
    "    # Ensure values are in valid range\n",
    "    images = torch.clamp(images, 0, 1)\n",
    "    \n",
    "    if not is_patches:\n",
    "        # For regular images, rearrange from [B, C, H, W] to [B, H, W, C]\n",
    "        images = rearrange(images, 'b c h w -> b h w c')\n",
    "        images = images.numpy()\n",
    "    \n",
    "    # Get number of images in batch\n",
    "    batch_size = images.shape[0]\n",
    "    \n",
    "    # Calculate grid dimensions\n",
    "    ncols = 4\n",
    "    nrows = (batch_size + ncols - 1) // ncols  # Ceiling division to ensure all images fit\n",
    "    \n",
    "    # Create figure with larger size\n",
    "    # Adjust these multipliers to change individual image size\n",
    "    plt.figure(figsize=(4 * ncols, 4 * nrows))  # Each image gets roughly 4x4 inches\n",
    "    \n",
    "    # Create subplot grid\n",
    "    for idx in range(batch_size):\n",
    "        plt.subplot(nrows, ncols, idx + 1)\n",
    "        plt.imshow(images[idx])\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Usage:\n",
    "# fig = show_batch(images, is_patches=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of channels (C): 3\n",
      "Height (H): 32\n",
      "Width (W): 32\n",
      "Patch size (patch_size): 2\n",
      "Number of patches (num_patches): 256\n",
      "Embedding dimension (emb_dim): 512\n",
      "Number of heads (num_heads): 8\n",
      "Head dimension: 64\n",
      "Number of transformer layers (num_layers): 8\n",
      "Hidden dimensions: 2048\n",
      "Number of classes: 100\n"
     ]
    }
   ],
   "source": [
    "C = images.shape[1]\n",
    "H = images.shape[2]\n",
    "W = images.shape[3]\n",
    "patch_size = 2\n",
    "num_patches = H*W // patch_size**2\n",
    "emb_dim = 512\n",
    "num_heads = 8\n",
    "num_layers = 8\n",
    "hidden_dim = 4*emb_dim\n",
    "dropout = 0.1\n",
    "drop_path_rate = 0.1\n",
    "n_classes = 100\n",
    "img_shape = (H,W)\n",
    "print(f\"Number of channels (C): {C}\")\n",
    "print(f\"Height (H): {H}\")\n",
    "print(f\"Width (W): {W}\")\n",
    "print(f\"Patch size (patch_size): {patch_size}\")\n",
    "print(f\"Number of patches (num_patches): {num_patches}\")\n",
    "print(f\"Embedding dimension (emb_dim): {emb_dim}\")\n",
    "print(f\"Number of heads (num_heads): {num_heads}\")\n",
    "print(f\"Head dimension: {emb_dim // num_heads}\")\n",
    "print(f\"Number of transformer layers (num_layers): {num_layers}\")\n",
    "print(f\"Hidden dimensions: {hidden_dim}\")\n",
    "print(f\"Number of classes: {n_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 3, 32, 32])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = show_batch(images.cpu(), is_patches=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# torch.set_float32_matmul_precision('high')\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class Patchify(nn.Module):\n",
    "#     def __init__(self, patch_size: int, C: int, img_shape: tuple, emb_dim: int):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             patch_size (int): Size of each patch (height and width).\n",
    "#             C (int): Number of input channels.\n",
    "#             img_shape (tuple): Shape of the input image (height, width).\n",
    "#             emb_dim (int): Dimension of the patch embeddings.\n",
    "#         \"\"\"\n",
    "#         super(Patchify, self).__init__()\n",
    "#         self.patch_size = patch_size\n",
    "#         self.C = C\n",
    "#         self.h, self.w = img_shape\n",
    "#         self.emb_dim = emb_dim\n",
    "        \n",
    "#         # Calculate the number of patches\n",
    "#         self.num_patches = (self.h // patch_size) * (self.w // patch_size)\n",
    "        \n",
    "#         # Learnable class token (shape: (1, emb_dim))\n",
    "#         self.class_token = nn.Parameter(torch.randn(1, emb_dim))\n",
    "        \n",
    "#         # Learnable positional embeddings (shape: (1, num_patches + 1, emb_dim))\n",
    "#         self.pos_emb = nn.Parameter(torch.randn(1, self.num_patches + 1, emb_dim))\n",
    "        \n",
    "#         self.norm = nn.LayerNorm(patch_size * patch_size * C)\n",
    "        \n",
    "#         # Linear projection layer (if patch embedding dimension != emb_dim)\n",
    "#         self.proj = nn.Linear(patch_size * patch_size * C, emb_dim) if patch_size * patch_size * C != emb_dim else nn.Identity()\n",
    "        \n",
    "#         self.patch_emb_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             x (torch.Tensor): Input image tensor of shape (batch_size, C, H, W).\n",
    "        \n",
    "#         Returns:\n",
    "#             torch.Tensor: Patch embeddings with class token and positional embeddings,\n",
    "#                           of shape (batch_size, num_patches + 1, emb_dim).\n",
    "#         \"\"\"\n",
    "#         # Patchify the image: (batch_size, C, H, W) -> (batch_size, num_patches, patch_size * patch_size * C)\n",
    "#         x = rearrange(x, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=self.patch_size, p2=self.patch_size)\n",
    "        \n",
    "#         # Project patch embeddings to the embedding dimension (if necessary)\n",
    "#         x = self.proj(x)  # Shape: (batch_size, num_patches, emb_dim)\n",
    "        \n",
    "#         # Get batch size dynamically\n",
    "#         batch_size = x.shape[0]\n",
    "        \n",
    "#         # Expand class token to match batch size: (1, emb_dim) -> (batch_size, 1, emb_dim)\n",
    "#         class_token = repeat(self.class_token, '1 d -> b 1 d', b=batch_size)\n",
    "        \n",
    "#         # Prepend the class token to the patch embeddings\n",
    "#         x = torch.cat([class_token, x], dim=1)  # Shape: (batch_size, num_patches + 1, emb_dim)\n",
    "        \n",
    "#         # Add positional embeddings\n",
    "#         x = x + self.pos_emb  # Shape: (batch_size, num_patches + 1, emb_dim)\n",
    "        \n",
    "#         x = self.patch_emb_dropout(x)\n",
    "        \n",
    "#         return x\n",
    "\n",
    "class Patchify(nn.Module):\n",
    "    def __init__(self, patch_size: int, C: int, img_shape: tuple, emb_dim: int, dropout: float):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patch_size (int): Size of each patch (height and width).\n",
    "            C (int): Number of input channels.\n",
    "            img_shape (tuple): Shape of the input image (height, width).\n",
    "            emb_dim (int): Dimension of the patch embeddings.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(Patchify, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.C = C\n",
    "        self.h, self.w = img_shape\n",
    "        self.emb_dim = emb_dim\n",
    "        \n",
    "        # Calculate the number of patches\n",
    "        self.num_patches = (self.h // patch_size) * (self.w // patch_size)\n",
    "        \n",
    "        # Learnable class token (shape: (1, emb_dim))\n",
    "        self.class_token = nn.Parameter(torch.randn(1, emb_dim))\n",
    "        \n",
    "        # Learnable positional embeddings (shape: (1, num_patches + 1, emb_dim))\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1, self.num_patches + 1, emb_dim))\n",
    "        \n",
    "        # Add LayerNorm before projection\n",
    "        self.norm = nn.LayerNorm(patch_size * patch_size * C)\n",
    "        \n",
    "        # Linear projection layer with sequential norm and projection\n",
    "        self.proj = nn.Sequential(\n",
    "            self.norm,\n",
    "            nn.Linear(patch_size * patch_size * C, emb_dim)\n",
    "        ) if patch_size * patch_size * C != emb_dim else nn.Identity()\n",
    "        \n",
    "        self.patch_emb_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Patchify the image\n",
    "        x = rearrange(x, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=self.patch_size, p2=self.patch_size)\n",
    "        \n",
    "        # Project patches\n",
    "        x = self.proj(x)\n",
    "        \n",
    "        # Get batch size dynamically\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Expand class token\n",
    "        class_token = repeat(self.class_token, '1 d -> b 1 d', b=batch_size)\n",
    "        \n",
    "        # Prepend class token\n",
    "        x = torch.cat([class_token, x], dim=1)\n",
    "        \n",
    "        # Add positional embeddings\n",
    "        x = x + self.pos_emb\n",
    "        \n",
    "        # Apply dropout\n",
    "        x = self.patch_emb_dropout(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patchify = Patchify(patch_size=patch_size, C=C, img_shape=(H,W), emb_dim=emb_dim).to(device)\n",
    "# patches = patchify(images)\n",
    "# patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From a scratch\n",
    "# import torch.nn.functional as F\n",
    "# class MHSA(nn.Module):\n",
    "#     def __init__(self, num_heads, emb_dim):\n",
    "#         super(MHSA, self).__init__()\n",
    "#         self.num_heads = num_heads\n",
    "#         self.emb_dim = emb_dim\n",
    "#         self.head_dim = emb_dim // num_heads \n",
    "#         self.wq = nn.Linear(emb_dim, emb_dim)\n",
    "#         self.wk = nn.Linear(emb_dim, emb_dim)\n",
    "#         self.wv = nn.Linear(emb_dim, emb_dim)\n",
    "#         self.attn_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         batch_size, num_patches_plus_1, emb_dim = x.shape\n",
    "#         q = self.wq(x) ## bs, num_patches + 1, emb_dim\n",
    "#         k = self.wk(x) ## bs, num_patches + 1, emb_dim\n",
    "#         v = self.wv(x) ## bs, num_patches + 1, emb_dim\n",
    "#         q = rearrange(q, 'bs patches_plus_1 (num_heads head_size) -> bs num_heads patches_plus_1 head_size', num_heads=self.num_heads, head_size=self.head_dim)\n",
    "#         k = rearrange(k, 'bs patches_plus_1 (num_heads head_size) -> bs num_heads patches_plus_1 head_size', num_heads=self.num_heads, head_size=self.head_dim)\n",
    "#         v = rearrange(v, 'bs patches_plus_1 (num_heads head_size) -> bs num_heads patches_plus_1 head_size', num_heads=self.num_heads, head_size=self.head_dim)\n",
    "#         attention_scores = q @ torch.transpose(k,-1,-2) * (self.head_dim ** -0.5)\n",
    "#         attention_scores = F.softmax(attention_scores, dim=-1)\n",
    "#         attention_scores = self.attn_dropout(attention_scores)\n",
    "#         output = attention_scores @ v\n",
    "#         output = rearrange(output, 'bs num_heads patch_size head_dim -> bs patch_size (head_dim num_heads)')\n",
    "#         # return q, k, v, output\n",
    "#         return output\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "\n",
    "class MHSA(nn.Module):\n",
    "    def __init__(self, num_heads, emb_dim, dropout):  # Add dropout parameter\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.emb_dim = emb_dim\n",
    "        self.head_dim = emb_dim // num_heads\n",
    "        \n",
    "        # Combine QKV into single matrix for efficiency\n",
    "        self.qkv = nn.Linear(emb_dim, 3 * emb_dim, bias=False)\n",
    "        self.proj = nn.Linear(emb_dim, emb_dim)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.proj_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, num_patches_plus_1, emb_dim = x.shape\n",
    "        \n",
    "        # Single matrix multiplication for Q, K, V\n",
    "        qkv = self.qkv(x)\n",
    "        qkv = rearrange(qkv, 'b n (three h d) -> three b h n d', \n",
    "                       three=3, h=self.num_heads)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # Use PyTorch's native scaled dot-product attention\n",
    "        output = F.scaled_dot_product_attention(\n",
    "            q, k, v,\n",
    "            dropout_p=self.attn_dropout.p if self.training else 0.0,\n",
    "            is_causal=False\n",
    "        )\n",
    "        \n",
    "        # Reshape output\n",
    "        output = rearrange(output, 'b h n d -> b n (h d)')\n",
    "        output = self.proj(output)\n",
    "        output = self.proj_dropout(output)\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mhsa = MHSA(num_heads, emb_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q, k, v, out = mhsa(patches)\n",
    "# print(q.shape, k.shape, v.shape, out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0. or not self.training:\n",
    "            return x\n",
    "        \n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()  # binarize\n",
    "        output = x.div(keep_prob) * random_tensor\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VITLayer(nn.Module):\n",
    "#     def __init__(self, num_heads, emb_dim, hidden_dim):\n",
    "#         super(VITLayer, self).__init__()\n",
    "#         self.attention_norm = nn.LayerNorm(emb_dim)\n",
    "#         self.ff_norm = nn.LayerNorm(emb_dim)\n",
    "        \n",
    "#         self.attention_block = MHSA(num_heads, emb_dim)\n",
    "#         self.ff_block = nn.Sequential(\n",
    "#             nn.Linear(emb_dim, hidden_dim),\n",
    "#             nn.GELU(),\n",
    "#             nn.Dropout(dropout),\n",
    "#             nn.Linear(hidden_dim, emb_dim),\n",
    "#             nn.Dropout(dropout)\n",
    "#         )\n",
    "        \n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = x + self.attention_block(self.attention_norm(x))\n",
    "#         x = x + self.ff_block(self.ff_norm(x))\n",
    "        \n",
    "#         return x\n",
    "\n",
    "# Modify your VITLayer to include drop_path\n",
    "class VITLayer(nn.Module):\n",
    "    def __init__(self, num_heads, emb_dim, hidden_dim, dropout, drop_path=0.):\n",
    "        super(VITLayer, self).__init__()\n",
    "        self.attention_norm = nn.LayerNorm(emb_dim)\n",
    "        self.ff_norm = nn.LayerNorm(emb_dim)\n",
    "        \n",
    "        self.attention_block = MHSA(num_heads, emb_dim, dropout)\n",
    "        self.ff_block = nn.Sequential(\n",
    "            nn.Linear(emb_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, emb_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attention_block(self.attention_norm(x)))\n",
    "        x = x + self.drop_path(self.ff_block(self.ff_norm(x)))\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vit_layer = VITLayer(num_heads=num_heads, emb_dim=emb_dim, hidden_dim=emb_dim*4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VIT(nn.Module):\n",
    "#     def __init__(self, \n",
    "#                  num_layers = num_layers, \n",
    "#                  num_heads = num_heads, \n",
    "#                  emb_dim = emb_dim, \n",
    "#                  hidden_dim = hidden_dim, \n",
    "#                  patch_size = patch_size,\n",
    "#                  C = C, \n",
    "#                  img_shape = img_shape,\n",
    "#                  n_classes = n_classes):\n",
    "#         super(VIT,self).__init__()\n",
    "#         self.patch_embedding = Patchify(patch_size=patch_size, C=C, img_shape=img_shape, emb_dim=emb_dim)\n",
    "#         self.xfls = nn.ModuleList([VITLayer(num_heads=num_heads, \n",
    "#                                                emb_dim=emb_dim, \n",
    "#                                                hidden_dim=hidden_dim) \n",
    "#                                   for i in range(num_layers)])\n",
    "#         self.norm = nn.LayerNorm(emb_dim)\n",
    "#         self.lin = nn.Linear(emb_dim, n_classes)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.patch_embedding(x)\n",
    "#         for layer in self.xfls:\n",
    "#             x = layer(x)\n",
    "            \n",
    "#         x = self.norm(x)\n",
    "#         logits = self.lin(x)\n",
    "#         return logits[:, 0]\n",
    "\n",
    "class VIT(nn.Module):\n",
    "    def __init__(self, num_layers, num_heads, emb_dim, hidden_dim, patch_size,\n",
    "                 C, img_shape, n_classes, dropout=0.3, drop_path_rate=0.1):\n",
    "        super(VIT, self).__init__()\n",
    "        self.patch_embedding = Patchify(patch_size=patch_size, C=C, \n",
    "                                      img_shape=img_shape, emb_dim=emb_dim, \n",
    "                                      dropout=dropout)\n",
    "        \n",
    "        # Initialize drop path rates (linearly increasing)\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, num_layers)]\n",
    "        self.xfls = nn.ModuleList([\n",
    "            VITLayer(\n",
    "                num_heads=num_heads,\n",
    "                emb_dim=emb_dim,\n",
    "                hidden_dim=hidden_dim,\n",
    "                dropout=dropout,\n",
    "                drop_path=dpr[i]\n",
    "            ) for i in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(emb_dim)\n",
    "        self.lin = nn.Linear(emb_dim, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.patch_embedding(x)\n",
    "        for layer in self.xfls:\n",
    "            x = layer(x)\n",
    "        x = self.norm(x)\n",
    "        x = x[:, 0]  # Get CLS token\n",
    "        x = self.lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vit = VIT(num_layers = num_layers, \n",
    "#                  num_heads = num_heads, \n",
    "#                  emb_dim = emb_dim, \n",
    "#                  hidden_dim = hidden_dim, \n",
    "#                  patch_size = patch_size,\n",
    "#                  C = C, \n",
    "#                  img_shape = img_shape,\n",
    "#                  n_classes = n_classes)\n",
    "vit = VIT(\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    emb_dim=emb_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    patch_size=patch_size,\n",
    "    C=C,\n",
    "    img_shape=img_shape,\n",
    "    n_classes=n_classes,\n",
    "    dropout=0.3,\n",
    "    drop_path_rate=0.1  # Start with this value\n",
    ")\n",
    "# Compile the model\n",
    "vit = torch.compile(vit)\n",
    "vit = vit.to(device)\n",
    "# vit.load_state_dict(torch.load('vit_weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_orig_mod.patch_embedding.class_token: 512 parameters\n",
      "_orig_mod.patch_embedding.pos_emb: 131,584 parameters\n",
      "_orig_mod.patch_embedding.norm.weight: 12 parameters\n",
      "_orig_mod.patch_embedding.norm.bias: 12 parameters\n",
      "_orig_mod.patch_embedding.proj.1.weight: 6,144 parameters\n",
      "_orig_mod.patch_embedding.proj.1.bias: 512 parameters\n",
      "_orig_mod.xfls.0.attention_norm.weight: 512 parameters\n",
      "_orig_mod.xfls.0.attention_norm.bias: 512 parameters\n",
      "_orig_mod.xfls.0.ff_norm.weight: 512 parameters\n",
      "_orig_mod.xfls.0.ff_norm.bias: 512 parameters\n",
      "_orig_mod.xfls.0.attention_block.qkv.weight: 786,432 parameters\n",
      "_orig_mod.xfls.0.attention_block.proj.weight: 262,144 parameters\n",
      "_orig_mod.xfls.0.attention_block.proj.bias: 512 parameters\n",
      "_orig_mod.xfls.0.ff_block.0.weight: 1,048,576 parameters\n",
      "_orig_mod.xfls.0.ff_block.0.bias: 2,048 parameters\n",
      "_orig_mod.xfls.0.ff_block.3.weight: 1,048,576 parameters\n",
      "_orig_mod.xfls.0.ff_block.3.bias: 512 parameters\n",
      "_orig_mod.xfls.1.attention_norm.weight: 512 parameters\n",
      "_orig_mod.xfls.1.attention_norm.bias: 512 parameters\n",
      "_orig_mod.xfls.1.ff_norm.weight: 512 parameters\n",
      "_orig_mod.xfls.1.ff_norm.bias: 512 parameters\n",
      "_orig_mod.xfls.1.attention_block.qkv.weight: 786,432 parameters\n",
      "_orig_mod.xfls.1.attention_block.proj.weight: 262,144 parameters\n",
      "_orig_mod.xfls.1.attention_block.proj.bias: 512 parameters\n",
      "_orig_mod.xfls.1.ff_block.0.weight: 1,048,576 parameters\n",
      "_orig_mod.xfls.1.ff_block.0.bias: 2,048 parameters\n",
      "_orig_mod.xfls.1.ff_block.3.weight: 1,048,576 parameters\n",
      "_orig_mod.xfls.1.ff_block.3.bias: 512 parameters\n",
      "_orig_mod.xfls.2.attention_norm.weight: 512 parameters\n",
      "_orig_mod.xfls.2.attention_norm.bias: 512 parameters\n",
      "_orig_mod.xfls.2.ff_norm.weight: 512 parameters\n",
      "_orig_mod.xfls.2.ff_norm.bias: 512 parameters\n",
      "_orig_mod.xfls.2.attention_block.qkv.weight: 786,432 parameters\n",
      "_orig_mod.xfls.2.attention_block.proj.weight: 262,144 parameters\n",
      "_orig_mod.xfls.2.attention_block.proj.bias: 512 parameters\n",
      "_orig_mod.xfls.2.ff_block.0.weight: 1,048,576 parameters\n",
      "_orig_mod.xfls.2.ff_block.0.bias: 2,048 parameters\n",
      "_orig_mod.xfls.2.ff_block.3.weight: 1,048,576 parameters\n",
      "_orig_mod.xfls.2.ff_block.3.bias: 512 parameters\n",
      "_orig_mod.xfls.3.attention_norm.weight: 512 parameters\n",
      "_orig_mod.xfls.3.attention_norm.bias: 512 parameters\n",
      "_orig_mod.xfls.3.ff_norm.weight: 512 parameters\n",
      "_orig_mod.xfls.3.ff_norm.bias: 512 parameters\n",
      "_orig_mod.xfls.3.attention_block.qkv.weight: 786,432 parameters\n",
      "_orig_mod.xfls.3.attention_block.proj.weight: 262,144 parameters\n",
      "_orig_mod.xfls.3.attention_block.proj.bias: 512 parameters\n",
      "_orig_mod.xfls.3.ff_block.0.weight: 1,048,576 parameters\n",
      "_orig_mod.xfls.3.ff_block.0.bias: 2,048 parameters\n",
      "_orig_mod.xfls.3.ff_block.3.weight: 1,048,576 parameters\n",
      "_orig_mod.xfls.3.ff_block.3.bias: 512 parameters\n",
      "_orig_mod.xfls.4.attention_norm.weight: 512 parameters\n",
      "_orig_mod.xfls.4.attention_norm.bias: 512 parameters\n",
      "_orig_mod.xfls.4.ff_norm.weight: 512 parameters\n",
      "_orig_mod.xfls.4.ff_norm.bias: 512 parameters\n",
      "_orig_mod.xfls.4.attention_block.qkv.weight: 786,432 parameters\n",
      "_orig_mod.xfls.4.attention_block.proj.weight: 262,144 parameters\n",
      "_orig_mod.xfls.4.attention_block.proj.bias: 512 parameters\n",
      "_orig_mod.xfls.4.ff_block.0.weight: 1,048,576 parameters\n",
      "_orig_mod.xfls.4.ff_block.0.bias: 2,048 parameters\n",
      "_orig_mod.xfls.4.ff_block.3.weight: 1,048,576 parameters\n",
      "_orig_mod.xfls.4.ff_block.3.bias: 512 parameters\n",
      "_orig_mod.xfls.5.attention_norm.weight: 512 parameters\n",
      "_orig_mod.xfls.5.attention_norm.bias: 512 parameters\n",
      "_orig_mod.xfls.5.ff_norm.weight: 512 parameters\n",
      "_orig_mod.xfls.5.ff_norm.bias: 512 parameters\n",
      "_orig_mod.xfls.5.attention_block.qkv.weight: 786,432 parameters\n",
      "_orig_mod.xfls.5.attention_block.proj.weight: 262,144 parameters\n",
      "_orig_mod.xfls.5.attention_block.proj.bias: 512 parameters\n",
      "_orig_mod.xfls.5.ff_block.0.weight: 1,048,576 parameters\n",
      "_orig_mod.xfls.5.ff_block.0.bias: 2,048 parameters\n",
      "_orig_mod.xfls.5.ff_block.3.weight: 1,048,576 parameters\n",
      "_orig_mod.xfls.5.ff_block.3.bias: 512 parameters\n",
      "_orig_mod.xfls.6.attention_norm.weight: 512 parameters\n",
      "_orig_mod.xfls.6.attention_norm.bias: 512 parameters\n",
      "_orig_mod.xfls.6.ff_norm.weight: 512 parameters\n",
      "_orig_mod.xfls.6.ff_norm.bias: 512 parameters\n",
      "_orig_mod.xfls.6.attention_block.qkv.weight: 786,432 parameters\n",
      "_orig_mod.xfls.6.attention_block.proj.weight: 262,144 parameters\n",
      "_orig_mod.xfls.6.attention_block.proj.bias: 512 parameters\n",
      "_orig_mod.xfls.6.ff_block.0.weight: 1,048,576 parameters\n",
      "_orig_mod.xfls.6.ff_block.0.bias: 2,048 parameters\n",
      "_orig_mod.xfls.6.ff_block.3.weight: 1,048,576 parameters\n",
      "_orig_mod.xfls.6.ff_block.3.bias: 512 parameters\n",
      "_orig_mod.xfls.7.attention_norm.weight: 512 parameters\n",
      "_orig_mod.xfls.7.attention_norm.bias: 512 parameters\n",
      "_orig_mod.xfls.7.ff_norm.weight: 512 parameters\n",
      "_orig_mod.xfls.7.ff_norm.bias: 512 parameters\n",
      "_orig_mod.xfls.7.attention_block.qkv.weight: 786,432 parameters\n",
      "_orig_mod.xfls.7.attention_block.proj.weight: 262,144 parameters\n",
      "_orig_mod.xfls.7.attention_block.proj.bias: 512 parameters\n",
      "_orig_mod.xfls.7.ff_block.0.weight: 1,048,576 parameters\n",
      "_orig_mod.xfls.7.ff_block.0.bias: 2,048 parameters\n",
      "_orig_mod.xfls.7.ff_block.3.weight: 1,048,576 parameters\n",
      "_orig_mod.xfls.7.ff_block.3.bias: 512 parameters\n",
      "_orig_mod.norm.weight: 512 parameters\n",
      "_orig_mod.norm.bias: 512 parameters\n",
      "_orig_mod.lin.weight: 51,200 parameters\n",
      "_orig_mod.lin.bias: 100 parameters\n",
      "Total trainable parameters: 25,397,884\n"
     ]
    }
   ],
   "source": [
    "## Calculate number of params\n",
    "total_params = 0\n",
    "for name, param in vit.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        num_params = param.numel()\n",
    "        print(f\"{name}: {num_params:,} parameters\")\n",
    "        total_params += num_params\n",
    "print(f\"Total trainable parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shwetank/code/diffusion/diffusion-env/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:167: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 100])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = vit(images)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6700, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Checking initialization and making sure loss is reasonable\n",
    "import torch.nn.functional as F\n",
    "loss = F.cross_entropy(out.to('cpu'), targets)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To continue training\n",
    "# vit.load_state_dict(torch.load('vit_weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 74\u001b[0m\n\u001b[1;32m     72\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     73\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 74\u001b[0m     running_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m running_train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[1;32m     77\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# Initialize accelerator\n",
    "accelerator = Accelerator(mixed_precision='bf16')\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 50\n",
    "max_lr = 4e-5  # Peak learning rate\n",
    "base_lr = max_lr/25  # Starting learning rate\n",
    "max_grad_norm = 1.0  # Add this here\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    vit.parameters(),\n",
    "    lr=base_lr,\n",
    "    weight_decay=5e-2,\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "\n",
    "\n",
    "# Calculate total steps\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "\n",
    "# Lists to store loss values for plotting\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# scheduler = ReduceLROnPlateau(\n",
    "#     optimizer, \n",
    "#     mode='min',\n",
    "#     factor=0.1,\n",
    "#     patience=5,\n",
    "#     min_lr=1e-6\n",
    "# )\n",
    "\n",
    "# Single scheduler instead of warmup + plateau\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=max_lr,\n",
    "    total_steps=total_steps,\n",
    "    pct_start=0.3,  # Spend 30% of time warming up\n",
    "    anneal_strategy='cos',\n",
    "    div_factor=25.0,\n",
    "    final_div_factor=1000.0\n",
    ")\n",
    "\n",
    "# Prepare model, dataloaders, and optimizer with accelerator\n",
    "vit, optimizer, train_loader, val_loader = accelerator.prepare(\n",
    "    vit, optimizer, train_loader, val_loader\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # Training phase\n",
    "    vit.train()\n",
    "    running_train_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = vit(images)\n",
    "        loss = F.cross_entropy(outputs, targets)\n",
    "        \n",
    "        # Use accelerator for backward pass\n",
    "        accelerator.backward(loss)\n",
    "        clip_grad_norm_(vit.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        running_train_loss += loss.item()\n",
    "    \n",
    "    train_loss = running_train_loss / len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    vit.eval()\n",
    "    running_val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, targets) in enumerate(val_loader):\n",
    "            outputs = vit(images)\n",
    "            loss = F.cross_entropy(outputs, targets)\n",
    "            running_val_loss += loss.item()\n",
    "    \n",
    "    val_loss = running_val_loss / len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Print epoch statistics\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "          f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, '\n",
    "          f'Time: {epoch_time:.2f} seconds')\n",
    "    \n",
    "        \n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f'Current learning rate: {current_lr:.2e}')\n",
    "\n",
    "# Save model weights (unwrap model first)\n",
    "unwrapped_model = accelerator.unwrap_model(vit)\n",
    "torch.save(unwrapped_model.state_dict(), 'vit_weights.pth')\n",
    "\n",
    "# Plot the final training and validation losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, 'b-', label='Training Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, 'r-', label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot both loss and accuracy\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(range(1, len(train_losses) + 1), train_losses, 'b-', label='Training Loss')\n",
    "ax1.plot(range(1, len(val_losses) + 1), val_losses, 'r-', label='Validation Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot(range(1, len(train_accuracies) + 1), train_accuracies, 'b-', label='Training Accuracy')\n",
    "ax2.plot(range(1, len(val_accuracies) + 1), val_accuracies, 'r-', label='Validation Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "# torch.save(vit.state_dict(), 'vit_weights.pth')\n",
    "def save_weights(model, path):\n",
    "    \"\"\"\n",
    "    Saves unwrapped model weights in bfloat16\n",
    "    \"\"\"\n",
    "    # Get the original model from the accelerator wrapper\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    \n",
    "    # Get raw state dict\n",
    "    raw_state_dict = unwrapped_model.state_dict()\n",
    "    \n",
    "    # Convert to bfloat16 and clean keys\n",
    "    clean_state_dict = {}\n",
    "    for key, value in raw_state_dict.items():\n",
    "        # Remove any _orig_mod prefix\n",
    "        clean_key = key.replace('_orig_mod.', '')\n",
    "        # Convert to bfloat16 if appropriate\n",
    "        clean_value = value.to(torch.bfloat16) if value.dtype in [torch.float32, torch.float16] else value\n",
    "        clean_state_dict[clean_key] = clean_value\n",
    "    \n",
    "    # Save the cleaned state dict\n",
    "    torch.save(clean_state_dict, path)\n",
    "\n",
    "# Save\n",
    "save_weights(vit, 'vit_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_test = VIT(\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    emb_dim=emb_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    patch_size=patch_size,\n",
    "    C=C,\n",
    "    img_shape=img_shape,\n",
    "    n_classes=n_classes,\n",
    "    dropout=0.3,\n",
    "    drop_path_rate=0.1  # Start with this value\n",
    ")\n",
    "vit_test.load_state_dict(torch.load('vit_weights.pth'))\n",
    "vit_test.to(device)\n",
    "vit_test.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Store predictions and labels if you need them later\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy on validation set: {accuracy:.2f}%')\n",
    "    \n",
    "    return accuracy, all_preds, all_labels\n",
    "\n",
    "# Run evaluation\n",
    "accuracy, predictions, true_labels = evaluate(vit_test, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset details\n",
    "print(f\"Number of validation samples: {len(val_loader.dataset)}\")\n",
    "print(f\"Number of classes: {len(val_loader.dataset.classes) if hasattr(val_loader.dataset, 'classes') else 'Unknown'}\")\n",
    "\n",
    "# Check distribution of predictions\n",
    "from collections import Counter\n",
    "pred_distribution = Counter(predictions)\n",
    "print(f\"Number of unique predicted classes: {len(pred_distribution)}\")\n",
    "print(f\"Min/Max predictions per class: {min(pred_distribution.values())}, {max(pred_distribution.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
